{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2974f450-71e3-4fdf-a828-0bbc90b57325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: https://huggingface.co/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M\n",
    "urls = [\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/0.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/1.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/2.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/3.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/4.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/5.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/6.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/7.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/8.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/9.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/10.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/11.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/12.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/13.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/14.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/15.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/16.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/17.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/18.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/19.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/20.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/21.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/22.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/23.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/24.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/25.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/26.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/27.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/28.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/29.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/30.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/31.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/32.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/33.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/34.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/35.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/36.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/37.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/38.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/39.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/40.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/41.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/42.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/43.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/44.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/45.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/46.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/47.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/48.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/49.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/50.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/51.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/52.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/53.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/54.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/55.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/56.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/57.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/58.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/59.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/60.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/61.parquet\",\"https://huggingface.co/api/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M/parquet/default/train/62.parquet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa22bb4d-ead0-472d-9b98-c18d4fdec439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 0.parquet\n",
      "Skipping 1.parquet\n",
      "Skipping 2.parquet\n",
      "Skipping 3.parquet\n",
      "Skipping 4.parquet\n",
      "Skipping 5.parquet\n",
      "Skipping 6.parquet\n",
      "Skipping 7.parquet\n",
      "Skipping 8.parquet\n",
      "Skipping 9.parquet\n",
      "Skipping 10.parquet\n",
      "Skipping 11.parquet\n",
      "Skipping 12.parquet\n",
      "Skipping 13.parquet\n",
      "Skipping 14.parquet\n",
      "Skipping 15.parquet\n",
      "Downloaded 16.parquet\n",
      "Skipping 17.parquet\n",
      "Skipping 18.parquet\n",
      "Skipping 19.parquet\n",
      "Skipping 20.parquet\n",
      "Skipping 21.parquet\n",
      "Skipping 22.parquet\n",
      "Skipping 23.parquet\n",
      "Skipping 24.parquet\n",
      "Skipping 25.parquet\n",
      "Skipping 26.parquet\n",
      "Skipping 27.parquet\n",
      "Skipping 28.parquet\n",
      "Downloaded 29.parquet\n",
      "Skipping 30.parquet\n",
      "Skipping 31.parquet\n",
      "Skipping 32.parquet\n",
      "Skipping 33.parquet\n",
      "Skipping 34.parquet\n",
      "Skipping 35.parquet\n",
      "Skipping 36.parquet\n",
      "Downloaded 37.parquet\n",
      "Skipping 38.parquet\n",
      "Skipping 39.parquet\n",
      "Skipping 40.parquet\n",
      "Skipping 41.parquet\n",
      "Skipping 42.parquet\n",
      "Skipping 43.parquet\n",
      "Skipping 44.parquet\n",
      "Skipping 45.parquet\n",
      "Skipping 46.parquet\n",
      "Skipping 47.parquet\n",
      "Skipping 48.parquet\n",
      "Skipping 49.parquet\n",
      "Skipping 50.parquet\n",
      "Skipping 51.parquet\n",
      "Skipping 52.parquet\n",
      "Skipping 53.parquet\n",
      "Skipping 54.parquet\n",
      "Skipping 55.parquet\n",
      "Skipping 56.parquet\n",
      "Skipping 57.parquet\n",
      "Skipping 58.parquet\n",
      "Skipping 59.parquet\n",
      "Skipping 60.parquet\n",
      "Skipping 61.parquet\n",
      "Skipping 62.parquet\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def download_file(url, folder):\n",
    "    filename = url.split('/')[-1]\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Skipping {filename}\")\n",
    "        return\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        # Open the file and write the content\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {filename}\")\n",
    "\n",
    "folder_name = \"dataset\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "for url in urls:\n",
    "    download_file(url, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74f7a20-0435-4d13-bba4-8bc9930e0434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/0.parquet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = list(map(lambda url: \"dataset/\" + url.split('/')[-1], urls))[:1]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1c25d7-0f86-4faf-aa8f-e65adc83fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import psycopg2\n",
    "pg_engine = psycopg2.connect('postgres://postgres:SecurePassword123!@localhost:5432/postgres')\n",
    "pg_engine.autocommit = True\n",
    "pg_session = pg_engine.cursor()\n",
    "binary_f = io.BytesIO(b\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57049cee-4c41-47ac-9ec7-b10e5bab95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset to Postgres\n",
    "import pyarrow.dataset as ds\n",
    "from pgpq import ArrowToPostgresBinaryEncoder\n",
    "\n",
    "def load_parquets(parquet_files, table_name):\n",
    "    print(f\"loading {len(parquet_files)} files\")\n",
    "    print(f\"loading files: {parquet_files}\")\n",
    "    dataset = ds.dataset(parquet_files)\n",
    "\n",
    "    encoder = ArrowToPostgresBinaryEncoder(dataset.schema)\n",
    "\n",
    "    pg_schema = encoder.schema()\n",
    "\n",
    "    tmp_table_name = \"_tmp_parquet_data\"\n",
    "    pg_schema_columns = [(col_name.replace('-', '_'), col) for col_name, col in pg_schema.columns]\n",
    "    typed_cols = [f'\"{col_name}\" {col.data_type.ddl()}' for col_name, col in pg_schema_columns]\n",
    "    cols = [col_name for col_name, _ in pg_schema_columns]\n",
    "    cols_joined = ','.join(cols)\n",
    "    typed_cols_joined = ','.join(typed_cols)\n",
    "    print(f\"Columns: {cols_joined}\")\n",
    "\n",
    "    ddl = f\"CREATE UNLOGGED TABLE {tmp_table_name} ({typed_cols_joined})\"\n",
    "\n",
    "    pg_session.execute(f\"DROP TABLE IF EXISTS {tmp_table_name}\")\n",
    "    pg_session.execute(ddl)\n",
    "    print(f\"pg schema {pg_schema}\")\n",
    "    print(f\"Assuming underlying postgres table was created with columns: {typed_cols} via a statement equivalent (or columnwise-type-castable) to'{ddl}'\")\n",
    "\n",
    "    binary_f.truncate(0)\n",
    "    binary_f.seek(0)\n",
    "    copy = binary_f\n",
    "    copy.write(encoder.write_header())\n",
    "    batches = dataset.to_batches()\n",
    "    count = 0\n",
    "    for i, batch in enumerate(batches):\n",
    "        print(f\"batch: {i} batch len: {len(batch)}\")\n",
    "        b = encoder.write_batch(batch)\n",
    "        copy.write(b)\n",
    "        count += len(batch)\n",
    "\n",
    "    copy.write(encoder.finish())\n",
    "    binary_f.seek(0)\n",
    "\n",
    "    print(f\"Copying dataset into postgres...\")\n",
    "    pg_session.execute(f'CREATE TABLE IF NOT EXISTS {table_name}({typed_cols_joined})')\n",
    "    pg_session.copy_expert(f'COPY \"{tmp_table_name}\" ({cols_joined}) FROM STDIN WITH (FORMAT BINARY)', binary_f)\n",
    "    pg_session.execute(f'INSERT INTO \"{table_name}\" SELECT * FROM \"{tmp_table_name}\"')\n",
    "    pg_session.execute(f'DROP TABLE \"{tmp_table_name}\"')\n",
    "    pg_session.execute(f'VACUUM FULL \"{table_name}\"')\n",
    "    pg_session.execute(f'ALTER TABLE {table_name} ALTER COLUMN text_embedding_ada_002_1536_embedding TYPE real[] USING text_embedding_ada_002_1536_embedding::real[]')\n",
    "    pg_session.execute(f'ALTER TABLE {table_name} ALTER COLUMN text_embedding_3_large_3072_embedding TYPE real[] USING text_embedding_3_large_3072_embedding::real[]')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6197657e-bd07-41a8-b47c-6751598337f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 1 files\n",
      "loading files: ['dataset/0.parquet']\n",
      "Columns: _id,title,text,text_embedding_ada_002_1536_embedding,text_embedding_3_large_3072_embedding\n",
      "pg schema PostgresSchema([('_id', Column(Text(), true)), ('title', Column(Text(), true)), ('text', Column(Text(), true)), ('text-embedding-ada-002-1536-embedding', Column(List(Column(Float4(), true)), true)), ('text-embedding-3-large-3072-embedding', Column(List(Column(Float8(), true)), true))])\n",
      "Assuming underlying postgres table was created with columns: ['\"_id\" TEXT', '\"title\" TEXT', '\"text\" TEXT', '\"text_embedding_ada_002_1536_embedding\" FLOAT4[]', '\"text_embedding_3_large_3072_embedding\" FLOAT8[]'] via a statement equivalent (or columnwise-type-castable) to'CREATE UNLOGGED TABLE _tmp_parquet_data (\"_id\" TEXT,\"title\" TEXT,\"text\" TEXT,\"text_embedding_ada_002_1536_embedding\" FLOAT4[],\"text_embedding_3_large_3072_embedding\" FLOAT8[])'\n",
      "batch: 0 batch len: 1000\n",
      "batch: 1 batch len: 1000\n",
      "batch: 2 batch len: 1000\n",
      "batch: 3 batch len: 1000\n",
      "batch: 4 batch len: 1000\n",
      "batch: 5 batch len: 1000\n",
      "batch: 6 batch len: 1000\n",
      "batch: 7 batch len: 1000\n",
      "batch: 8 batch len: 1000\n",
      "batch: 9 batch len: 1000\n",
      "batch: 10 batch len: 1000\n",
      "batch: 11 batch len: 1000\n",
      "batch: 12 batch len: 1000\n",
      "batch: 13 batch len: 1000\n",
      "batch: 14 batch len: 1000\n",
      "batch: 15 batch len: 874\n",
      "Copying dataset into postgres...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15874"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_parquets(filenames, 'openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab93c846-ddaf-41c0-acb1-f5802a275df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<dbpedia:HMS_Upholder_(P37)>',\n",
       " 'HMS Upholder (P37)',\n",
       " 'HMS Upholder (P37) was a Royal Navy U-class submarine built by Vickers-Armstrong at Barrow-in-Furness.  She was laid down on 30 October 1939, launched on 8 July 1940 by Mrs. Doris Thompson, wife of a director of the builders. The submarine was commissioned on 31 October 1940. She was one of four U-class submarines which had two external torpedo tubes at the bows in addition to the 4 internal ones fitted to all boats.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample from dataset\n",
    "query = '''\n",
    "    SELECT\n",
    "        _id,\n",
    "        title,\n",
    "        text\n",
    "    FROM\n",
    "        openai\n",
    "    ORDER BY\n",
    "        RANDOM()\n",
    "    LIMIT 1\n",
    "'''\n",
    "pg_session.execute(query)\n",
    "pg_session.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bca11eec-3825-485c-a1a7-3d686931b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate \"ground truth\" dataset\n",
    "NEAREST_NEIGHBOR_COUNT = 10\n",
    "SAMPLE_SIZE_COUNT = 10\n",
    "\n",
    "query = f'''\n",
    "    SELECT\n",
    "        q._id AS query_id,\n",
    "        ARRAY_AGG(b._id) AS nearest_ids\n",
    "    FROM (\n",
    "        SELECT\n",
    "            _id,\n",
    "            text_embedding_3_large_3072_embedding\n",
    "        FROM\n",
    "            openai\n",
    "        ORDER BY\n",
    "            RANDOM()\n",
    "        LIMIT {SAMPLE_SIZE_COUNT}\n",
    "    ) q\n",
    "    JOIN LATERAL (\n",
    "        SELECT\n",
    "            _id,\n",
    "            text_embedding_3_large_3072_embedding\n",
    "        FROM\n",
    "            openai\n",
    "        ORDER BY\n",
    "            q.text_embedding_3_large_3072_embedding <-> text_embedding_3_large_3072_embedding\n",
    "        LIMIT {NEAREST_NEIGHBOR_COUNT}\n",
    "    ) b\n",
    "    ON TRUE\n",
    "    GROUP BY\n",
    "        q._id\n",
    "'''\n",
    "pg_session.execute(query)\n",
    "truth_array = pg_session.fetchall() # Contains array of (ID, nearest IDs) tuples\n",
    "query_ids = [t[0] for t in truth_array]\n",
    "truth_dict = {query_id: set(nn) for query_id, nn in truth_array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79dfba73-dbfa-4f24-b6d3-ae065d58ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Open AI Matryoshka embeddings\n",
    "query = '''\n",
    "    CREATE OR REPLACE FUNCTION normalize_array(arr REAL[])\n",
    "    RETURNS REAL[] AS $$\n",
    "    DECLARE\n",
    "        magnitude REAL := 0;\n",
    "        normalized_arr REAL[];\n",
    "    BEGIN\n",
    "        -- Calculate the magnitude of the array\n",
    "        SELECT sqrt(sum(val * val)) INTO magnitude\n",
    "        FROM unnest(arr) AS dt(val);\n",
    "    \n",
    "        -- Check if magnitude is zero to avoid division by zero\n",
    "        IF magnitude = 0 THEN\n",
    "            RETURN arr;\n",
    "        END IF;\n",
    "    \n",
    "        -- Normalize the array\n",
    "        SELECT array_agg(val / magnitude) INTO normalized_arr\n",
    "        FROM unnest(arr) AS dt(val);\n",
    "    \n",
    "        RETURN normalized_arr;\n",
    "    END;\n",
    "    $$ LANGUAGE plpgsql;\n",
    "    ALTER TABLE openai ADD COLUMN v256 REAL[];\n",
    "    ALTER TABLE openai ADD COLUMN v1024 REAL[];\n",
    "'''\n",
    "try:\n",
    "    pg_session.execute(query)\n",
    "    pg_session.execute('''\n",
    "        UPDATE openai SET v256 = text_embedding_3_large_3072_embedding[1:256];\n",
    "        UPDATE openai SET v1024 = text_embedding_3_large_3072_embedding[1:1024];\n",
    "        UPDATE openai SET v256 = normalize_array(v256);\n",
    "        UPDATE openai SET v1024 = normalize_array(v1024);\n",
    "    ''')\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374b9a7-1b4d-4205-a431-df7e8b151366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PQ embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a810cbe-ed20-43c6-8d98-e885a4656b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recall utility functions\n",
    "def get_query(column):\n",
    "    query = f'''\n",
    "        SELECT\n",
    "            q._id AS query_id,\n",
    "            ARRAY_AGG(b._id) AS truth_ids\n",
    "        FROM (\n",
    "            SELECT\n",
    "                _id,\n",
    "                {column}\n",
    "            FROM\n",
    "                openai\n",
    "            WHERE\n",
    "                _id = ANY(%s)\n",
    "        ) q\n",
    "        JOIN LATERAL (\n",
    "            SELECT\n",
    "                _id,\n",
    "                {column}\n",
    "            FROM\n",
    "                openai\n",
    "            ORDER BY\n",
    "                q.{column} <-> {column}\n",
    "            LIMIT {NEAREST_NEIGHBOR_COUNT}\n",
    "        ) b\n",
    "        ON TRUE\n",
    "        GROUP BY\n",
    "            q._id\n",
    "    '''\n",
    "    return query\n",
    "\n",
    "def get_recall(column):\n",
    "    query = get_query(column)\n",
    "    pg_session.execute(query, (query_ids,))\n",
    "    answers = pg_session.fetchall()\n",
    "    recall = sum([len(truth_dict[query_id].intersection(nn)) for query_id, nn in answers]) / (NEAREST_NEIGHBOR_COUNT * SAMPLE_SIZE_COUNT)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "027946ef-ac51-4b35-b486-a0ac1c3311ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58\n",
      "0.86\n"
     ]
    }
   ],
   "source": [
    "# Generate recall for Open AI embeddings\n",
    "print(get_recall('v256'))\n",
    "print(get_recall('v1024'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8bb8e-eb26-4d53-b4e5-2d5b5e21b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recall for PQ embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
