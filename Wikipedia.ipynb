{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd24cf9",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2974f450-71e3-4fdf-a828-0bbc90b57325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: https://huggingface.co/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-small-512-100K\n",
    "urls = [\"https://huggingface.co/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-small-512-100K/raw/main/data/train-00000-of-00001.parquet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa22bb4d-ead0-472d-9b98-c18d4fdec439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded train-00000-of-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def download_file(url, folder):\n",
    "    filename = url.split('/')[-1]\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Skipping {filename}\")\n",
    "        return\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        # Open the file and write the content\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                f.write(chunk)\n",
    "        print(f\"Downloaded {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {filename}\")\n",
    "\n",
    "folder_name = \"dataset\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "for url in urls:\n",
    "    download_file(url, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e74f7a20-0435-4d13-bba4-8bc9930e0434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/train-00000-of-00001.parquet']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = list(map(lambda url: \"dataset/\" + url.split('/')[-1], urls))[:1]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babb397e",
   "metadata": {},
   "source": [
    "# Connect to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import psycopg2\n",
    "pg_engine = psycopg2.connect('postgres://postgres:SecurePassword123!@localhost:5432/postgres')\n",
    "pg_engine.autocommit = True\n",
    "pg_session = pg_engine.cursor()\n",
    "binary_f = io.BytesIO(b\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf64c1ab",
   "metadata": {},
   "source": [
    "# Load dataset to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57049cee-4c41-47ac-9ec7-b10e5bab95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset to Postgres\n",
    "import pyarrow.dataset as ds\n",
    "from pgpq import ArrowToPostgresBinaryEncoder\n",
    "\n",
    "def load_parquets(parquet_files, table_name):\n",
    "    print(f\"loading {len(parquet_files)} files\")\n",
    "    print(f\"loading files: {parquet_files}\")\n",
    "    dataset = ds.dataset(parquet_files)\n",
    "\n",
    "    encoder = ArrowToPostgresBinaryEncoder(dataset.schema)\n",
    "\n",
    "    pg_schema = encoder.schema()\n",
    "\n",
    "    tmp_table_name = \"_tmp_parquet_data\"\n",
    "    pg_schema_columns = [(col_name.replace('-', '_'), col) for col_name, col in pg_schema.columns]\n",
    "    typed_cols = [f'\"{col_name}\" {col.data_type.ddl()}' for col_name, col in pg_schema_columns]\n",
    "    cols = [col_name for col_name, _ in pg_schema_columns]\n",
    "    cols_joined = ','.join(cols)\n",
    "    typed_cols_joined = ','.join(typed_cols)\n",
    "    print(f\"Columns: {cols_joined}\")\n",
    "\n",
    "    ddl = f\"CREATE UNLOGGED TABLE {tmp_table_name} ({typed_cols_joined})\"\n",
    "\n",
    "    pg_session.execute(f\"DROP TABLE IF EXISTS {tmp_table_name}\")\n",
    "    pg_session.execute(ddl)\n",
    "    print(f\"pg schema {pg_schema}\")\n",
    "    print(f\"Assuming underlying postgres table was created with columns: {typed_cols} via a statement equivalent (or columnwise-type-castable) to'{ddl}'\")\n",
    "\n",
    "    binary_f.truncate(0)\n",
    "    binary_f.seek(0)\n",
    "    copy = binary_f\n",
    "    copy.write(encoder.write_header())\n",
    "    batches = dataset.to_batches()\n",
    "    count = 0\n",
    "    for i, batch in enumerate(batches):\n",
    "        print(f\"batch: {i} batch len: {len(batch)}\")\n",
    "        b = encoder.write_batch(batch)\n",
    "        copy.write(b)\n",
    "        count += len(batch)\n",
    "\n",
    "    copy.write(encoder.finish())\n",
    "    binary_f.seek(0)\n",
    "\n",
    "    print(f\"Copying dataset into postgres...\")\n",
    "    pg_session.execute(f'CREATE TABLE IF NOT EXISTS {table_name}({typed_cols_joined})')\n",
    "    pg_session.copy_expert(f'COPY \"{tmp_table_name}\" ({cols_joined}) FROM STDIN WITH (FORMAT BINARY)', binary_f)\n",
    "    pg_session.execute(f'INSERT INTO \"{table_name}\" SELECT * FROM \"{tmp_table_name}\"')\n",
    "    pg_session.execute(f'DROP TABLE \"{tmp_table_name}\"')\n",
    "    pg_session.execute(f'VACUUM FULL \"{table_name}\"')\n",
    "    pg_session.execute(f'ALTER TABLE {table_name} ALTER COLUMN text_embedding_3_small_512_embedding TYPE real[] USING text_embedding_3_small_512_embedding::real[]')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6197657e-bd07-41a8-b47c-6751598337f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filenames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m load_parquets(\u001b[43mfilenames\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filenames' is not defined"
     ]
    }
   ],
   "source": [
    "load_parquets(filenames, 'openai')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9372758d",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab93c846-ddaf-41c0-acb1-f5802a275df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<dbpedia:Giovanni_di_Giovanni>',\n",
       " 'Giovanni di Giovanni',\n",
       " 'Giovanni di Giovanni (c. 1350 – May 7, 1365?) is one of the youngest victims of the  campaign against sodomy waged in Florence since the Middle Ages.He was convicted by the Podestà court of being the passive partner of a number of different men. He was labeled \"a public and notorious passive sodomite.\" His punishment was to be paraded on the back of an ass, then to be publicly castrated.')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample from dataset\n",
    "query = '''\n",
    "    SELECT\n",
    "        _id,\n",
    "        title,\n",
    "        text\n",
    "    FROM\n",
    "        openai\n",
    "    ORDER BY\n",
    "        RANDOM()\n",
    "    LIMIT 1\n",
    "'''\n",
    "pg_session.execute(query)\n",
    "pg_session.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930012c2",
   "metadata": {},
   "source": [
    "# Generate ground truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69cdd915",
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateTable",
     "evalue": "relation \"openai_query\" already exists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateTable\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m    CREATE TABLE openai_query (id TEXT, vector REAL[], nn_ids TEXT[]);\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    CREATE TABLE openai_reference (id TEXT, vector REAL[]);\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    INSERT INTO openai_query (id, vector) SELECT _id, text_embedding_3_small_512_embedding FROM openai ORDER BY RANDOM() LIMIT 100;\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    INSERT INTO openai_reference SELECT _id, text_embedding_3_small_512_embedding FROM openai WHERE _id NOT IN (SELECT id FROM openai_query);\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mpg_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mDuplicateTable\u001b[0m: relation \"openai_query\" already exists\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    CREATE TABLE openai_query (id TEXT, vector REAL[], nn_ids TEXT[]);\n",
    "    CREATE TABLE openai_reference (id TEXT, vector REAL[]);\n",
    "    INSERT INTO openai_query (id, vector) SELECT _id, text_embedding_3_small_512_embedding FROM openai ORDER BY RANDOM() LIMIT 100;\n",
    "    INSERT INTO openai_reference SELECT _id, text_embedding_3_small_512_embedding FROM openai WHERE _id NOT IN (SELECT id FROM openai_query);\n",
    "\"\"\"\n",
    "\n",
    "pg_session.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca11eec-3825-485c-a1a7-3d686931b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate \"ground truth\" dataset\n",
    "query = f'''\n",
    "    UPDATE openai_query\n",
    "    SET nn_ids = nearest_ids.ids\n",
    "    FROM (\n",
    "        SELECT\n",
    "            q.id AS id,\n",
    "            ARRAY_AGG(r.id) AS ids\n",
    "        FROM openai_query q\n",
    "        JOIN LATERAL (\n",
    "            SELECT\n",
    "                id,\n",
    "                vector\n",
    "            FROM\n",
    "                openai_reference r\n",
    "            ORDER BY\n",
    "                q.vector <-> r.vector\n",
    "            LIMIT 10\n",
    "        ) r\n",
    "        ON TRUE\n",
    "        GROUP BY\n",
    "            q.id\n",
    "    ) AS nearest_ids\n",
    "    WHERE\n",
    "        openai_query.id = nearest_ids.id\n",
    "'''\n",
    "pg_session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c34e19",
   "metadata": {},
   "source": [
    "# Generate copy (for testing index on original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e6d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    ALTER TABLE openai_reference ADD COLUMN vector512 REAL[];\n",
    "    UPDATE openai_reference SET vector512 = vector;\n",
    "\"\"\"\n",
    "pg_session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75dd12",
   "metadata": {},
   "source": [
    "# Generate Matryoshka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79dfba73-dbfa-4f24-b6d3-ae065d58ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Open AI Matryoshka embeddings\n",
    "query = '''\n",
    "    CREATE OR REPLACE FUNCTION normalize_array(arr REAL[])\n",
    "    RETURNS REAL[] AS $$\n",
    "    DECLARE\n",
    "        magnitude REAL := 0;\n",
    "        normalized_arr REAL[];\n",
    "    BEGIN\n",
    "        -- Calculate the magnitude of the array\n",
    "        SELECT sqrt(sum(val * val)) INTO magnitude\n",
    "        FROM unnest(arr) AS dt(val);\n",
    "    \n",
    "        -- Check if magnitude is zero to avoid division by zero\n",
    "        IF magnitude = 0 THEN\n",
    "            RETURN arr;\n",
    "        END IF;\n",
    "    \n",
    "        -- Normalize the array\n",
    "        SELECT array_agg(val / magnitude) INTO normalized_arr\n",
    "        FROM unnest(arr) AS dt(val);\n",
    "    \n",
    "        RETURN normalized_arr;\n",
    "    END;\n",
    "    $$ LANGUAGE plpgsql;\n",
    "    ALTER TABLE openai_reference ADD COLUMN vector256 REAL[];\n",
    "'''\n",
    "try:\n",
    "    pg_session.execute(query)\n",
    "    pg_session.execute('''\n",
    "        UPDATE openai_reference SET vector256 = vector256[1:256];\n",
    "        UPDATE openai_reference SET vector256 = normalize_array(vector256);\n",
    "    ''')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8783ebd",
   "metadata": {},
   "source": [
    "# Generate PQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b374b9a7-1b4d-4205-a431-df7e8b151366",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT drop_quantization('openai_reference', 'vector');\n",
    "    SELECT quantize_table('openai_reference', 'vector', 256, 16, 'l2sq', 10000);\n",
    "\"\"\"\n",
    "pg_session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f2afe",
   "metadata": {},
   "source": [
    "# Create Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b258c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    CREATE INDEX ON openai_reference USING lantern_hnsw (vector256) WITH (M=32, ef_construction=64, ef=128, dim=256);\n",
    "\"\"\"\n",
    "pg_session.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    CREATE INDEX ON openai_reference USING lantern_hnsw (vector512) WITH (M=32, ef_construction=64, ef=128, dim=256);\n",
    "\"\"\"\n",
    "pg_session.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332043c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    CREATE INDEX ON openai_reference USING lantern_hnsw (vector) WITH (M=32, ef_construction=64, ef=128, pq=True);\n",
    "\"\"\"\n",
    "pg_session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d11f4e",
   "metadata": {},
   "source": [
    "# Check recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(column):\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            q.id AS query_id,\n",
    "            nn_ids AS truth_ids,\n",
    "            ARRAY_AGG(b.id) AS reference_ids\n",
    "        FROM\n",
    "            openai_query q\n",
    "        JOIN LATERAL (\n",
    "            SELECT\n",
    "                id\n",
    "            FROM\n",
    "                openai_reference r\n",
    "            ORDER BY\n",
    "                q.vector <-> r.{column}\n",
    "            LIMIT 10\n",
    "        ) r\n",
    "        ON TRUE\n",
    "        GROUP BY\n",
    "            q.id\n",
    "    \"\"\"\n",
    "    pg_session.execute(query)\n",
    "    data = pg_session.fetchall()\n",
    "    recall = 0\n",
    "    for row in data:\n",
    "        truth_ids = set(row[1])\n",
    "        reference_ids = set(row[2])\n",
    "        recall += len(truth_ids.intersection(reference_ids)) / len(truth_ids)\n",
    "    return recall / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027946ef-ac51-4b35-b486-a0ac1c3311ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58\n",
      "0.86\n"
     ]
    }
   ],
   "source": [
    "# Generate recall for Open AI embeddings\n",
    "get_recall('vector512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8bb8e-eb26-4d53-b4e5-2d5b5e21b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recall for PQ embeddings\n",
    "get_recall('vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recall for Matryoshka embeddings\n",
    "get_recall('vector256')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
